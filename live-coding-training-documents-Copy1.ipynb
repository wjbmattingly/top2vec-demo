{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f3d518-eb56-4979-8f24-5bbbe6840807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ede8bca-7c53-4bfc-8ac7-41a37f5c7157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON, Thabo Simon</td>\n",
       "      <td>An ANCYL member who was shot and severely inju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBOTT, Montaigne</td>\n",
       "      <td>A member of the SADF who was severely injured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABDUL WAHAB, Zakier</td>\n",
       "      <td>A member of QIBLA who disappeared in September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABRAHAM, Nzaliseko Christopher</td>\n",
       "      <td>A COSAS supporter who was kicked and beaten wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABRAHAMS, Achmat Fardiel</td>\n",
       "      <td>Was shot and blinded in one eye by members of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21742</th>\n",
       "      <td>ZWENI, Ernest</td>\n",
       "      <td>One of two South African Police (SAP) members ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21743</th>\n",
       "      <td>ZWENI, Lebuti</td>\n",
       "      <td>An ANC supporter who was shot dead by a named ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21744</th>\n",
       "      <td>ZWENI, Louis</td>\n",
       "      <td>Was shot dead in Tokoza, Transvaal, on 22 May ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21745</th>\n",
       "      <td>ZWENI, Mpantesa William</td>\n",
       "      <td>His home was lost in an arson attack by Witdoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21746</th>\n",
       "      <td>ZWENI, Xolile Milton</td>\n",
       "      <td>A Transkei Defence Force (TDF) soldier who was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21747 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                names  \\\n",
       "0                  AARON, Thabo Simon   \n",
       "1                   ABBOTT, Montaigne   \n",
       "2                 ABDUL WAHAB, Zakier   \n",
       "3      ABRAHAM, Nzaliseko Christopher   \n",
       "4            ABRAHAMS, Achmat Fardiel   \n",
       "...                               ...   \n",
       "21742                   ZWENI, Ernest   \n",
       "21743                   ZWENI, Lebuti   \n",
       "21744                    ZWENI, Louis   \n",
       "21745         ZWENI, Mpantesa William   \n",
       "21746            ZWENI, Xolile Milton   \n",
       "\n",
       "                                            descriptions  \n",
       "0      An ANCYL member who was shot and severely inju...  \n",
       "1      A member of the SADF who was severely injured ...  \n",
       "2      A member of QIBLA who disappeared in September...  \n",
       "3      A COSAS supporter who was kicked and beaten wi...  \n",
       "4      Was shot and blinded in one eye by members of ...  \n",
       "...                                                  ...  \n",
       "21742  One of two South African Police (SAP) members ...  \n",
       "21743  An ANC supporter who was shot dead by a named ...  \n",
       "21744  Was shot dead in Tokoza, Transvaal, on 22 May ...  \n",
       "21745  His home was lost in an arson attack by Witdoe...  \n",
       "21746  A Transkei Defence Force (TDF) soldier who was...  \n",
       "\n",
       "[21747 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"data/vol7.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75a73970-3112-4203-a6c6-f7131afb1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.descriptions.tolist()\n",
    "names = df.names.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89d96727-a245-48b4-8172-6fd4761c7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = []\n",
    "for i, name in enumerate(names):\n",
    "    unique_name = f\"{i:04}_{name}\"\n",
    "    unique_names.append(unique_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceffb08e-dc5c-410f-96ec-add1330efb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Was interrogated, tortured and killed by AZAPO members along with five other scholars in Soweto, Johannesburg, on 1 August 1986. The incident was sparked off by the burning of the house of an AZAPO leader for which the youths were believed to have been responsible. Three perpetrators were refused amnesty, and one was granted amnesty.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [d.replace(\"See \", \"\") for d in docs]\n",
    "docs = [re.sub(r\"\\([^()]*\\)\", \"\", d).replace(\" .\", \".\") for d in docs]\n",
    "docs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62c4bd5c-72ec-487e-91af-3bbc0a2e5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top2Vec\n",
      "\n",
      "    Creates jointly embedded topic, document and word vectors.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    documents: List of str\n",
      "        Input corpus, should be a list of strings.\n",
      "\n",
      "    min_count: int (Optional, default 50)\n",
      "        Ignores all words with total frequency lower than this. For smaller\n",
      "        corpora a smaller min_count will be necessary.\n",
      "\n",
      "    ngram_vocab: bool (Optional, default False)\n",
      "        Add phrases to topic descriptions.\n",
      "\n",
      "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
      "        to the vocabulary.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    ngram_vocab_args: dict (Optional, default None)\n",
      "        Pass custom arguments to gensim phrases.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    embedding_model: string or callable\n",
      "        This will determine which model is used to generate the document and\n",
      "        word embeddings. The valid string options are:\n",
      "\n",
      "            * doc2vec\n",
      "            * universal-sentence-encoder\n",
      "            * universal-sentence-encoder-large\n",
      "            * universal-sentence-encoder-multilingual\n",
      "            * universal-sentence-encoder-multilingual-large\n",
      "            * distiluse-base-multilingual-cased\n",
      "            * all-MiniLM-L6-v2\n",
      "            * paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "        For large data sets and data sets with very unique vocabulary doc2vec\n",
      "        could produce better results. This will train a doc2vec model from\n",
      "        scratch. This method is language agnostic. However multiple languages\n",
      "        will not be aligned.\n",
      "\n",
      "        Using the universal sentence encoder options will be much faster since\n",
      "        those are pre-trained and efficient models. The universal sentence\n",
      "        encoder options are suggested for smaller data sets. They are also\n",
      "        good options for large data sets that are in English or in languages\n",
      "        covered by the multilingual model. It is also suggested for data sets\n",
      "        that are multilingual.\n",
      "\n",
      "        For more information on universal-sentence-encoder options visit:\n",
      "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
      "\n",
      "        The SBERT pre-trained sentence transformer options are\n",
      "        distiluse-base-multilingual-cased,\n",
      "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
      "\n",
      "        The distiluse-base-multilingual-cased and\n",
      "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
      "        datasets and languages that are not\n",
      "        covered by the multilingual universal sentence encoder. The\n",
      "        transformer is significantly slower than the universal sentence\n",
      "        encoder options(except for the large options).\n",
      "\n",
      "        For more information on SBERT options visit:\n",
      "        https://www.sbert.net/docs/pretrained_models.html\n",
      "\n",
      "        If passing a callable embedding_model note that it will not be saved\n",
      "        when saving a top2vec model. After loading such a saved top2vec model\n",
      "        the set_embedding_model method will need to be called and the same\n",
      "        embedding_model callable used during training must be passed to it.\n",
      "\n",
      "    embedding_model_path: string (Optional)\n",
      "        Pre-trained embedding models will be downloaded automatically by\n",
      "        default. However they can also be uploaded from a file that is in the\n",
      "        location of embedding_model_path.\n",
      "\n",
      "        Warning: the model at embedding_model_path must match the\n",
      "        embedding_model parameter type.\n",
      "\n",
      "    embedding_batch_size: int (default=32)\n",
      "        Batch size for documents being embedded.\n",
      "\n",
      "    split_documents: bool (default False)\n",
      "        If set to True, documents will be split into parts before embedding.\n",
      "        After embedding the multiple document part embeddings will be averaged\n",
      "        to create a single embedding per document. This is useful when documents\n",
      "        are very large or when the embedding model has a token limit.\n",
      "\n",
      "        Document chunking or a senticizer can be used for document splitting.\n",
      "\n",
      "    document_chunker: string or callable (default 'sequential')\n",
      "        This will break the document into chunks. The valid string options are:\n",
      "\n",
      "            * sequential\n",
      "            * random\n",
      "\n",
      "        The sequential chunker will split the document into chunks of specified\n",
      "        length and ratio of overlap. This is the recommended method.\n",
      "\n",
      "        The random chunking option will take random chunks of specified length\n",
      "        from the document. These can overlap and should be thought of as\n",
      "        sampling chunks with replacement from the document.\n",
      "\n",
      "        If a callable is passed it must take as input a list of tokens of\n",
      "        a document and return a list of strings representing the resulting\n",
      "        document chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    chunk_length: int (default 100)\n",
      "        The number of tokens per document chunk if using the document chunker\n",
      "        string options.\n",
      "\n",
      "    max_num_chunks: int (Optional)\n",
      "        The maximum number of chunks generated per document if using the\n",
      "        document chunker string options.\n",
      "\n",
      "    chunk_overlap_ratio: float (default 0.5)\n",
      "        Only applies to the 'sequential' document chunker.\n",
      "\n",
      "        Fraction of overlapping tokens between sequential chunks. A value of\n",
      "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
      "        previous chunk.\n",
      "\n",
      "    chunk_len_coverage_ratio: float (default 1.0)\n",
      "        Only applies to the 'random' document chunker option.\n",
      "\n",
      "        Proportion of token length that will be covered by chunks. Default\n",
      "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
      "        the document. This does not mean all tokens will be covered since\n",
      "        chunks can be overlapping.\n",
      "\n",
      "    sentencizer: callable (Optional)\n",
      "        A sentincizer callable can be passed. The input should be a string\n",
      "        representing the document and the output should be a list of strings\n",
      "        representing the document sentence chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    speed: string (Optional, default 'learn')\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        It will determine how fast the model takes to train. The\n",
      "        fast-learn option is the fastest and will generate the lowest quality\n",
      "        vectors. The learn option will learn better quality vectors but take\n",
      "        a longer time to train. The deep-learn option will learn the best\n",
      "        quality vectors but will take significant time to train. The valid\n",
      "        string speed options are:\n",
      "        \n",
      "            * fast-learn\n",
      "            * learn\n",
      "            * deep-learn\n",
      "\n",
      "    use_corpus_file: bool (Optional, default False)\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        Setting use_corpus_file to True can sometimes provide speedup for\n",
      "        large datasets when multiple worker threads are available. Documents\n",
      "        are still passed to the model as a list of str, the model will create\n",
      "        a temporary corpus file for training.\n",
      "\n",
      "    document_ids: List of str, int (Optional)\n",
      "        A unique value per document that will be used for referring to\n",
      "        documents in search results. If ids are not given to the model, the\n",
      "        index of each document in the original corpus will become the id.\n",
      "\n",
      "    keep_documents: bool (Optional, default True)\n",
      "        If set to False documents will only be used for training and not saved\n",
      "        as part of the model. This will reduce model size. When using search\n",
      "        functions only document ids will be returned, not the actual\n",
      "        documents.\n",
      "\n",
      "    workers: int (Optional)\n",
      "        The amount of worker threads to be used in training the model. Larger\n",
      "        amount will lead to faster training.\n",
      "    \n",
      "    tokenizer: callable (Optional, default None)\n",
      "        Override the default tokenization method. If None then\n",
      "        gensim.utils.simple_preprocess will be used.\n",
      "\n",
      "        Tokenizer must take a document and return a list of tokens.\n",
      "\n",
      "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
      "        If using an embedding model other than doc2vec, use the model's\n",
      "        tokenizer for document embedding. If set to True the tokenizer, either\n",
      "        default or passed callable will be used to tokenize the text to\n",
      "        extract the vocabulary for word embedding.\n",
      "\n",
      "    umap_args: dict (Optional, default None)\n",
      "        Pass custom arguments to UMAP.\n",
      "\n",
      "    hdbscan_args: dict (Optional, default None)\n",
      "        Pass custom arguments to HDBSCAN.\n",
      "    \n",
      "    verbose: bool (Optional, default True)\n",
      "        Whether to print status data during training.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffcf4f06-66c4-4a6a-a601-3c6db878fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 11:50:53,680 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-01 11:50:55,256 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-01 11:51:30,225 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-01 11:51:50,870 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-01 11:51:53,368 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(docs, speed=\"fast-learn\", document_ids=unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c390e302-dcb9-4694-8031-bc5af744395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 6372_MAHLAMVU, Douglas, Score: 0.9940192103385925\n",
      "-----------\n",
      "An ANC supporter who was detained for seven months at Lusikisiki, Transkei, in 1960, for his involvement in the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 2423_FAKU, Mhlabuvukile, Score: 0.9931095838546753\n",
      "-----------\n",
      "Was deported from Kokstad to Sekhukhuneland during the Pondoland revolt in 1960. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 13770_NAKWA, Mzayifani Alfred, Score: 0.9928367137908936\n",
      "-----------\n",
      "Was detained for a month by members of the SAP in Bizana, Transkei, in June 1960, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 9696_MFUYWA, Sigwebo, Score: 0.9925034046173096\n",
      "-----------\n",
      "Was shot dead by members of the SAP on 6 June 1960 in Flagstaff, Transkei, during the Pondoland revolt.\n",
      "-----------\n",
      "\n",
      "Document: 19223_SIPIKA, Ntamehlo, Score: 0.9921572208404541\n",
      "-----------\n",
      "An iKongo member who was shot dead by members of the SAP on 6 June 1960 in Bizana, Transkei, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 1612_DINWA, Qhekwane, Score: 0.9920756220817566\n",
      "-----------\n",
      "An ANC supporter who was arrested in Bizana, Transkei, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 18770_SHKITA, Zanyokwe, Score: 0.9920054078102112\n",
      "-----------\n",
      "Was beaten by members of the SAP in Bizana, Transkei, in 1960 during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 9388_MDLAMLA, Bambaliphi, Score: 0.9918415546417236\n",
      "-----------\n",
      "An iKongo member who was imprisoned in June 1960 in Lusikisiki, Transkei, for his involvement in the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 16686_NYAMAMBI, Mvunyelwa, Score: 0.9917508363723755\n",
      "-----------\n",
      "An iKongo member who was detained and beaten in January 1961 in Lusikisiki, Transkei, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 16551_NXASANA, Ndovela, Score: 0.9916064739227295\n",
      "-----------\n",
      "Was severely beaten at Lusikisiki, Transkei, in June 1960 during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=2, num_docs=10)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59c8c373-1ff0-4b78-9dd2-8f4353aade0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0000_AARON, Thabo Simon', '0001_ABBOTT, Montaigne',\n",
       "       '0002_ABDUL WAHAB, Zakier', ..., '21744_ZWENI, Louis',\n",
       "       '21745_ZWENI, Mpantesa William', '21746_ZWENI, Xolile Milton'],\n",
       "      dtype='<U85')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3135a50-69e0-4053-8794-7b77945b18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f60cc731-9afb-46d5-8bf7-f6b5be54393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names2 = []\n",
    "for name in unique_names:\n",
    "    name = name.replace(\", \", \"_\").replace(\" \", \"_\")\n",
    "    unique_names2.append(name)\n",
    "\n",
    "\n",
    "unique_names_array = np.asarray(unique_names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6cef067-2a08-4916-8d63-54b9506e0222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0000_AARON_Thabo_Simon', '0001_ABBOTT_Montaigne',\n",
       "       '0002_ABDUL_WAHAB_Zakier', ..., '21744_ZWENI_Louis',\n",
       "       '21745_ZWENI_Mpantesa_William', '21746_ZWENI_Xolile_Milton'],\n",
       "      dtype='<U84')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_names_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19e0ee6e-d179-4e58-995f-c38af13c0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.\n",
    "\n",
    "s = unique_names_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c493384f-43bb-440c-8f12-d0cfe4f8b232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0000_AARON_Thabo_Simon', '0001_ABBOTT_Montaigne',\n",
       "       '0002_ABDUL_WAHAB_Zakier', ..., '21744_ZWENI_Louis',\n",
       "       '21745_ZWENI_Mpantesa_William', '21746_ZWENI_Xolile_Milton'],\n",
       "      dtype='<U84')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eca0e60-78e8-40fe-9ca7-a0140ba07777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 6372_MAHLAMVU_Douglas, Score: 0.9940192103385925\n",
      "-----------\n",
      "An ANC supporter who was detained for seven months at Lusikisiki, Transkei, in 1960, for his involvement in the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 2423_FAKU_Mhlabuvukile, Score: 0.9931095838546753\n",
      "-----------\n",
      "Was deported from Kokstad to Sekhukhuneland during the Pondoland revolt in 1960. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 13770_NAKWA_Mzayifani_Alfred, Score: 0.9928367137908936\n",
      "-----------\n",
      "Was detained for a month by members of the SAP in Bizana, Transkei, in June 1960, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 9696_MFUYWA_Sigwebo, Score: 0.9925034046173096\n",
      "-----------\n",
      "Was shot dead by members of the SAP on 6 June 1960 in Flagstaff, Transkei, during the Pondoland revolt.\n",
      "-----------\n",
      "\n",
      "Document: 19223_SIPIKA_Ntamehlo, Score: 0.9921572208404541\n",
      "-----------\n",
      "An iKongo member who was shot dead by members of the SAP on 6 June 1960 in Bizana, Transkei, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 1612_DINWA_Qhekwane, Score: 0.9920756220817566\n",
      "-----------\n",
      "An ANC supporter who was arrested in Bizana, Transkei, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 18770_SHKITA_Zanyokwe, Score: 0.9920054078102112\n",
      "-----------\n",
      "Was beaten by members of the SAP in Bizana, Transkei, in 1960 during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 9388_MDLAMLA_Bambaliphi, Score: 0.9918415546417236\n",
      "-----------\n",
      "An iKongo member who was imprisoned in June 1960 in Lusikisiki, Transkei, for his involvement in the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 16686_NYAMAMBI_Mvunyelwa, Score: 0.9917508363723755\n",
      "-----------\n",
      "An iKongo member who was detained and beaten in January 1961 in Lusikisiki, Transkei, during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n",
      "Document: 16551_NXASANA_Ndovela, Score: 0.9916064739227295\n",
      "-----------\n",
      "Was severely beaten at Lusikisiki, Transkei, in June 1960 during the Pondoland revolt. See Pondoland revolt. \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=2, num_docs=10)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e855414-daa3-4e0d-aef5-230b5ffe5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93257c86-d8e1-4ff1-a19a-eb107ccdb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 12:03:46,631 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-01 12:03:48,466 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-01 12:04:24,555 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-01 12:04:32,732 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-01 12:04:33,327 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model2 = Top2Vec(docs, document_ids=unique_names, speed=\"fast-learn\", keep_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a8b8a55-f3c8-4f4e-9a4c-4546c62b1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "151b103f-94d8-41a0-ac23-ef3baeab99b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 17950_RULASHE, Dumile Drimond, Score: 0.9801660776138306\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 16047_NOVOLO, Gabriel, Score: 0.9737259149551392\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 19227_SIPOYA, Jarius Mokotedi, Score: 0.9725156426429749\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 11823_MONNANA, Moitlobo Ruth, Score: 0.9721013307571411\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 6685_MAKGALE, Christopher Ntshimane, Score: 0.9701863527297974\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 20328_TSHIKORORO, Ramaano Selwyn, Score: 0.9688754081726074\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 17812_RANTAO, Lydia Mmasethunya, Score: 0.9674817323684692\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 6195_MAGANADISA, Jacob, Score: 0.9674534797668457\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 8344_MAUMELA, Tshifihiwa Anthony, Score: 0.9669753313064575\n",
      "-----------\n",
      "-----------\n",
      "\n",
      "Document: 11539_MOKWENA, Wilson Kgashane, Score: 0.9662195444107056\n",
      "-----------\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_scores, document_ids = model2.search_documents_by_topic(topic_num=2, num_docs=10)\n",
    "for score, doc_id in zip(document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    # print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3e186-fec0-4abd-85cd-2fc80a7abae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
