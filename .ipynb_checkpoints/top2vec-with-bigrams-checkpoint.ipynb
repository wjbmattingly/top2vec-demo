{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "793efb0c-2f3d-4c7d-9dab-e4cb32e4157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e88e8f6-58c0-40cc-a548-b4f196016984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON, Thabo Simon</td>\n",
       "      <td>An ANCYL member who was shot and severely inju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBOTT, Montaigne</td>\n",
       "      <td>A member of the SADF who was severely injured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABDUL WAHAB, Zakier</td>\n",
       "      <td>A member of QIBLA who disappeared in September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABRAHAM, Nzaliseko Christopher</td>\n",
       "      <td>A COSAS supporter who was kicked and beaten wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABRAHAMS, Achmat Fardiel</td>\n",
       "      <td>Was shot and blinded in one eye by members of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21742</th>\n",
       "      <td>ZWENI, Ernest</td>\n",
       "      <td>One of two South African Police (SAP) members ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21743</th>\n",
       "      <td>ZWENI, Lebuti</td>\n",
       "      <td>An ANC supporter who was shot dead by a named ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21744</th>\n",
       "      <td>ZWENI, Louis</td>\n",
       "      <td>Was shot dead in Tokoza, Transvaal, on 22 May ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21745</th>\n",
       "      <td>ZWENI, Mpantesa William</td>\n",
       "      <td>His home was lost in an arson attack by Witdoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21746</th>\n",
       "      <td>ZWENI, Xolile Milton</td>\n",
       "      <td>A Transkei Defence Force (TDF) soldier who was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21747 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                names  \\\n",
       "0                  AARON, Thabo Simon   \n",
       "1                   ABBOTT, Montaigne   \n",
       "2                 ABDUL WAHAB, Zakier   \n",
       "3      ABRAHAM, Nzaliseko Christopher   \n",
       "4            ABRAHAMS, Achmat Fardiel   \n",
       "...                               ...   \n",
       "21742                   ZWENI, Ernest   \n",
       "21743                   ZWENI, Lebuti   \n",
       "21744                    ZWENI, Louis   \n",
       "21745         ZWENI, Mpantesa William   \n",
       "21746            ZWENI, Xolile Milton   \n",
       "\n",
       "                                            descriptions  \n",
       "0      An ANCYL member who was shot and severely inju...  \n",
       "1      A member of the SADF who was severely injured ...  \n",
       "2      A member of QIBLA who disappeared in September...  \n",
       "3      A COSAS supporter who was kicked and beaten wi...  \n",
       "4      Was shot and blinded in one eye by members of ...  \n",
       "...                                                  ...  \n",
       "21742  One of two South African Police (SAP) members ...  \n",
       "21743  An ANC supporter who was shot dead by a named ...  \n",
       "21744  Was shot dead in Tokoza, Transvaal, on 22 May ...  \n",
       "21745  His home was lost in an arson attack by Witdoe...  \n",
       "21746  A Transkei Defence Force (TDF) soldier who was...  \n",
       "\n",
       "[21747 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"data/vol7.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9c9506f-77f7-4b4f-8dfc-02e62fc9912e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Was interrogated, tortured and killed by AZAPO members along with five other scholars in Soweto, Johannesburg, on 1 August 1986. The incident was sparked off by the burning of the house of an AZAPO leader for which the youths were believed to have been responsible. Three perpetrators were refused amnesty, and one was granted amnesty.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df.descriptions.tolist()\n",
    "docs = [d.replace(\"See \", \"\") for d in docs]\n",
    "docs = [re.sub(r\"\\([^()]*\\)\", \"\", d).replace(\" .\", \".\") for d in docs]\n",
    "docs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b25348c-8335-42ce-9dd6-094fb9dbc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa04d083-18ff-4bac-b21e-c5c80c2646c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top2Vec\n",
      "\n",
      "    Creates jointly embedded topic, document and word vectors.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    documents: List of str\n",
      "        Input corpus, should be a list of strings.\n",
      "\n",
      "    min_count: int (Optional, default 50)\n",
      "        Ignores all words with total frequency lower than this. For smaller\n",
      "        corpora a smaller min_count will be necessary.\n",
      "\n",
      "    ngram_vocab: bool (Optional, default False)\n",
      "        Add phrases to topic descriptions.\n",
      "\n",
      "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
      "        to the vocabulary.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    ngram_vocab_args: dict (Optional, default None)\n",
      "        Pass custom arguments to gensim phrases.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    embedding_model: string or callable\n",
      "        This will determine which model is used to generate the document and\n",
      "        word embeddings. The valid string options are:\n",
      "\n",
      "            * doc2vec\n",
      "            * universal-sentence-encoder\n",
      "            * universal-sentence-encoder-large\n",
      "            * universal-sentence-encoder-multilingual\n",
      "            * universal-sentence-encoder-multilingual-large\n",
      "            * distiluse-base-multilingual-cased\n",
      "            * all-MiniLM-L6-v2\n",
      "            * paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "        For large data sets and data sets with very unique vocabulary doc2vec\n",
      "        could produce better results. This will train a doc2vec model from\n",
      "        scratch. This method is language agnostic. However multiple languages\n",
      "        will not be aligned.\n",
      "\n",
      "        Using the universal sentence encoder options will be much faster since\n",
      "        those are pre-trained and efficient models. The universal sentence\n",
      "        encoder options are suggested for smaller data sets. They are also\n",
      "        good options for large data sets that are in English or in languages\n",
      "        covered by the multilingual model. It is also suggested for data sets\n",
      "        that are multilingual.\n",
      "\n",
      "        For more information on universal-sentence-encoder options visit:\n",
      "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
      "\n",
      "        The SBERT pre-trained sentence transformer options are\n",
      "        distiluse-base-multilingual-cased,\n",
      "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
      "\n",
      "        The distiluse-base-multilingual-cased and\n",
      "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
      "        datasets and languages that are not\n",
      "        covered by the multilingual universal sentence encoder. The\n",
      "        transformer is significantly slower than the universal sentence\n",
      "        encoder options(except for the large options).\n",
      "\n",
      "        For more information on SBERT options visit:\n",
      "        https://www.sbert.net/docs/pretrained_models.html\n",
      "\n",
      "        If passing a callable embedding_model note that it will not be saved\n",
      "        when saving a top2vec model. After loading such a saved top2vec model\n",
      "        the set_embedding_model method will need to be called and the same\n",
      "        embedding_model callable used during training must be passed to it.\n",
      "\n",
      "    embedding_model_path: string (Optional)\n",
      "        Pre-trained embedding models will be downloaded automatically by\n",
      "        default. However they can also be uploaded from a file that is in the\n",
      "        location of embedding_model_path.\n",
      "\n",
      "        Warning: the model at embedding_model_path must match the\n",
      "        embedding_model parameter type.\n",
      "\n",
      "    embedding_batch_size: int (default=32)\n",
      "        Batch size for documents being embedded.\n",
      "\n",
      "    split_documents: bool (default False)\n",
      "        If set to True, documents will be split into parts before embedding.\n",
      "        After embedding the multiple document part embeddings will be averaged\n",
      "        to create a single embedding per document. This is useful when documents\n",
      "        are very large or when the embedding model has a token limit.\n",
      "\n",
      "        Document chunking or a senticizer can be used for document splitting.\n",
      "\n",
      "    document_chunker: string or callable (default 'sequential')\n",
      "        This will break the document into chunks. The valid string options are:\n",
      "\n",
      "            * sequential\n",
      "            * random\n",
      "\n",
      "        The sequential chunker will split the document into chunks of specified\n",
      "        length and ratio of overlap. This is the recommended method.\n",
      "\n",
      "        The random chunking option will take random chunks of specified length\n",
      "        from the document. These can overlap and should be thought of as\n",
      "        sampling chunks with replacement from the document.\n",
      "\n",
      "        If a callable is passed it must take as input a list of tokens of\n",
      "        a document and return a list of strings representing the resulting\n",
      "        document chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    chunk_length: int (default 100)\n",
      "        The number of tokens per document chunk if using the document chunker\n",
      "        string options.\n",
      "\n",
      "    max_num_chunks: int (Optional)\n",
      "        The maximum number of chunks generated per document if using the\n",
      "        document chunker string options.\n",
      "\n",
      "    chunk_overlap_ratio: float (default 0.5)\n",
      "        Only applies to the 'sequential' document chunker.\n",
      "\n",
      "        Fraction of overlapping tokens between sequential chunks. A value of\n",
      "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
      "        previous chunk.\n",
      "\n",
      "    chunk_len_coverage_ratio: float (default 1.0)\n",
      "        Only applies to the 'random' document chunker option.\n",
      "\n",
      "        Proportion of token length that will be covered by chunks. Default\n",
      "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
      "        the document. This does not mean all tokens will be covered since\n",
      "        chunks can be overlapping.\n",
      "\n",
      "    sentencizer: callable (Optional)\n",
      "        A sentincizer callable can be passed. The input should be a string\n",
      "        representing the document and the output should be a list of strings\n",
      "        representing the document sentence chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    speed: string (Optional, default 'learn')\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        It will determine how fast the model takes to train. The\n",
      "        fast-learn option is the fastest and will generate the lowest quality\n",
      "        vectors. The learn option will learn better quality vectors but take\n",
      "        a longer time to train. The deep-learn option will learn the best\n",
      "        quality vectors but will take significant time to train. The valid\n",
      "        string speed options are:\n",
      "        \n",
      "            * fast-learn\n",
      "            * learn\n",
      "            * deep-learn\n",
      "\n",
      "    use_corpus_file: bool (Optional, default False)\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        Setting use_corpus_file to True can sometimes provide speedup for\n",
      "        large datasets when multiple worker threads are available. Documents\n",
      "        are still passed to the model as a list of str, the model will create\n",
      "        a temporary corpus file for training.\n",
      "\n",
      "    document_ids: List of str, int (Optional)\n",
      "        A unique value per document that will be used for referring to\n",
      "        documents in search results. If ids are not given to the model, the\n",
      "        index of each document in the original corpus will become the id.\n",
      "\n",
      "    keep_documents: bool (Optional, default True)\n",
      "        If set to False documents will only be used for training and not saved\n",
      "        as part of the model. This will reduce model size. When using search\n",
      "        functions only document ids will be returned, not the actual\n",
      "        documents.\n",
      "\n",
      "    workers: int (Optional)\n",
      "        The amount of worker threads to be used in training the model. Larger\n",
      "        amount will lead to faster training.\n",
      "    \n",
      "    tokenizer: callable (Optional, default None)\n",
      "        Override the default tokenization method. If None then\n",
      "        gensim.utils.simple_preprocess will be used.\n",
      "\n",
      "        Tokenizer must take a document and return a list of tokens.\n",
      "\n",
      "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
      "        If using an embedding model other than doc2vec, use the model's\n",
      "        tokenizer for document embedding. If set to True the tokenizer, either\n",
      "        default or passed callable will be used to tokenize the text to\n",
      "        extract the vocabulary for word embedding.\n",
      "\n",
      "    umap_args: dict (Optional, default None)\n",
      "        Pass custom arguments to UMAP.\n",
      "\n",
      "    hdbscan_args: dict (Optional, default None)\n",
      "        Pass custom arguments to HDBSCAN.\n",
      "    \n",
      "    verbose: bool (Optional, default True)\n",
      "        Whether to print status data during training.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print (Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ceee01ba-f7cd-4eeb-9f1d-6ad4f74ccc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 11:35:27,702 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-06-30 11:35:29,341 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-06-30 11:43:40,496 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-06-30 11:43:47,985 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-06-30 11:43:50,396 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(docs, speed=\"deep-learn\", ngram_vocab=\"True\", embedding_model=\"doc2vec\", workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78382374-7746-47e7-b585-38b9c6f4fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2620"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35fde18f-1345-4bcb-8429-bc09fb3690f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915\n",
      "1705\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "unigrams = []\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "other = []\n",
    "for word in model.vocab:\n",
    "    size = len(word.split())\n",
    "    if size == 1:\n",
    "        unigrams.append(word)\n",
    "    elif size == 2:\n",
    "        bigrams.append(word)\n",
    "    elif size == 3:\n",
    "        trigrams.append(worod)\n",
    "    else:\n",
    "        other.append(word)\n",
    "print (len(unigrams))\n",
    "print (len(bigrams))\n",
    "print (len(trigrams))\n",
    "print (len(other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "954a5e9e-b2ad-41d1-8aaa-3c7931c861b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "282304dd-a802-426e-b5bb-a15f43c96f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbots poort',\n",
       " 'abducted along',\n",
       " 'aberdeen cape',\n",
       " 'about hours',\n",
       " 'about infiltration',\n",
       " 'accused him',\n",
       " 'acknowledge their',\n",
       " 'acquitted due',\n",
       " 'acting with',\n",
       " 'action committee']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b56b86-14f5-4747-ab88-2ca123ae6c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 15070, Score: 0.9640305638313293\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 14710, Score: 0.9617973566055298\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 15060, Score: 0.9610453844070435\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 11002, Score: 0.9605767726898193\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 20 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 15128, Score: 0.9604917764663696\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 14767, Score: 0.9598067998886108\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 16173, Score: 0.9596930742263794\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 15034, Score: 0.9590734243392944\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters at Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 13129, Score: 0.9589765667915344\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 15108, Score: 0.9581484794616699\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=0, num_docs=10)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6088f94-64ff-4a6a-94e7-f04a165f37b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[664 336 324 280 272 246 232 224 214 212 195 194 189 181 176 165 164 148\n",
      " 140 138 136 134 129 129 125 125 124 124 124 122 122 121 120 119 119 118\n",
      " 118 118 118 116 114 114 112 111 111 109 107 107 106 105 104 104 102 101\n",
      " 100 100 100  99  99  99  98  98  97  97  95  94  94  93  93  93  93  92\n",
      "  92  92  90  90  90  89  89  89  89  88  88  88  87  87  87  86  85  85\n",
      "  83  83  83  83  83  82  82  82  82  80  80  80  80  78  77  77  76  76\n",
      "  76  76  76  76  75  75  74  74  74  74  73  73  73  73  72  71  71  71\n",
      "  70  70  70  70  69  69  69  69  68  68  67  67  67  67  67  66  66  66\n",
      "  66  65  65  65  64  64  63  63  63  63  63  63  63  62  62  61  61  61\n",
      "  61  61  61  60  60  60  60  59  59  59  58  58  58  58  58  57  57  57\n",
      "  57  57  57  56  55  55  55  55  55  54  54  54  54  54  53  53  52  52\n",
      "  52  52  51  50  50  50  50  49  48  48  48  47  47  46  46  45  44  44\n",
      "  44  43  43  43  43  42  41  41  41  41  40  39  38  38  38  37  37  37\n",
      "  37  36  36  35  35  35  34  34  34  33  31  31  31  30  30  29  29  29\n",
      "  28  28  27  27  27  25  25  25  24  24  23  21  19  18  18  18  12  10]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print (topic_sizes)\n",
    "print (topic_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c52ce7d-a6b6-43c2-86b1-46f1314cbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9d14d92-3895-4318-bd78-db64e3a6b2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Words: ['sonkombo ndwedwe' 'sonkombo arson' 'ogunjini ndwedwe'\n",
      " 'thafamasi ndwedwe' 'hosiyana ndwedwe' 'arson attacks' 'ndwedwe kwazulu'\n",
      " 'sonkombo' 'physical attacks' 'speaking men' 'men carrying' 'burnt down'\n",
      " 'intense public' 'green factions' 'chief leabua' 'contralesa chief'\n",
      " 'chief lerumo' 'men wearing' 'appointed chief' 'chief lebogo'\n",
      " 'chief mankuroane' 'chief mapela' 'chief mhlabunzima' 'summoned chief'\n",
      " 'katlehong transvaal' 'chief jack' 'chief rule' 'security forces'\n",
      " 'ndwedwe' 'chief kd' 'intense political' 'whose house'\n",
      " 'completely destroyed' 'ekuthuleni attacks' 'mshayazafe inanda'\n",
      " 'kwamthethwa kwazulu' 'kwashange kwazulu' 'ezakheni kwazulu'\n",
      " 'ashdown kwazulu' 'mevamhlophe kwazulu' 'three applications'\n",
      " 'amaoti inanda' 'mashayazafe inanda' 'madadeni kwazulu' 'ohlange inanda'\n",
      " 'howick natal' 'sundumbili kwazulu' 'zindophe kwazulu' 'ngonweni inanda'\n",
      " 'apla attacks']\n",
      "1\n",
      "Words: ['into kwandebele' 'vlaklaagte kwandebele' 'tweefontein kwandebele'\n",
      " 'vaalbank kwandebele' 'moutse kwandebele' 'siyabuswa kwandebele'\n",
      " 'incorporation into' 'kwandebele' 'over incorporation'\n",
      " 'imbokodo vigilantes' 'dennilton kwandebele' 're incorporation'\n",
      " 'resisting incorporation' 'forceful incorporation' 'moutse' 'imbokodo'\n",
      " 'siyabuswa' 'incorporation' 'siyabuswa community' 'fierce resistance'\n",
      " 'suppress resistance' 'braklaagte into' 'into crowded' 'coerced into'\n",
      " 'go into' 'split into' 'gone into' 'hambanathi into'\n",
      " 'resisted incorporation' 'burst into' 'imbokodo vigilante' 'resistance'\n",
      " 'amasolomzi vigilantes' 'took place' 'themselves against'\n",
      " 'retaliating against' 'emergency regulations' 'nietverdiend ambush'\n",
      " 'over access' 'amasinyora vigilantes' 'hostels over' 'drove over'\n",
      " 'hiding place' 'against unita' 'vigilantes' 'under emergency'\n",
      " 'independence' 'dennilton' 'directed against' 'sproposed independence']\n"
     ]
    }
   ],
   "source": [
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print (num)\n",
    "    print (f\"Words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52df21-df6f-4ae3-b40f-9532dad4a249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
